{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 1: Downloading and preprocessing the twitter data for cities with the highest COVID \n",
    "#case counts\n",
    "\n",
    "import csv\n",
    "import json\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "import datetime\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "import string\n",
    "import operator\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "\n",
    "\n",
    "# Two reference documents, one with records of cumulative COVID cases by county, \n",
    "#and county seats(municipalities) by county\n",
    "\n",
    "covidhandle=open('time_series_covid19_confirmed_US.csv')\n",
    "covidcsv=csv.reader(covidhandle)\n",
    "\n",
    "\n",
    "seatshandle=open('CountySeats.csv')\n",
    "seatscsv=csv.reader(seatshandle)\n",
    "\n",
    "#Convert table of Counties and County Seats to a lookup table\n",
    "#because COVID data is by county, but twitter users are more\n",
    "#likely to list their location by city than county\n",
    "\n",
    "seatslookup=dict()\n",
    "for i in seatscsv:\n",
    "    seatslookup[i[0]]=i[1]\n",
    "\n",
    "\n",
    "#Function that will find the dates where COVID cases crossed certain\n",
    "#thresholds in different counties. Cutoff is counties with at least\n",
    "#100,000 total cases\n",
    "#After analysis, only 3 time periods are used to try to make the periods easier to \n",
    "#differentiate\n",
    "\n",
    "def linegrab(L):\n",
    "    if float(L[-1])>120000:\n",
    "        K1,K10,K30,K75,K100=0,0,0,0,0\n",
    "        \n",
    "\n",
    "        for i,ct in enumerate(L[11:]):\n",
    "            if K1==0:\n",
    "                if float(ct)>1000:\n",
    "                    K1=i\n",
    "            elif K10==0:\n",
    "                if float(ct)>10000:\n",
    "                    K10=i\n",
    "            elif K30==0:\n",
    "                if float(ct)>30000:\n",
    "                    K30=i\n",
    "            elif K75==0:\n",
    "                if float(ct)>60000:\n",
    "                    K75=i\n",
    "            elif K100==0:\n",
    "                if float(ct)>100000:\n",
    "                    K100=i\n",
    "\n",
    "        #return [K1,K10,K30,K75,K100]\n",
    "        return [K1,K10,K30,K100]\n",
    "    else:\n",
    "        return []\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "#First 11 fields are location data, daily COVID reports start after\n",
    "#First if clause takes dates, linegrab function finds the milestone\n",
    "#dates if the county is over 100K, and records them in a dictionary\n",
    "#that has the city (looked up from county seats) and milestone dates\n",
    "        \n",
    "spikes=[]\n",
    "\n",
    "for i,line in enumerate(covidcsv):\n",
    "    if i==0:\n",
    "        cdates=line[11:]       \n",
    "    else:\n",
    "        milestone=linegrab(line)\n",
    "        if len(milestone)>0:\n",
    "            msdate=[]\n",
    "            for m in milestone:\n",
    "                msdate.append(cdates[m])\n",
    "            spikes.append({'city':seatslookup[line[5]],'county':line[5],'state':line[6],'dates':msdate})\n",
    "\n",
    "covidhandle.close()\n",
    "seatshandle.close()\n",
    "\n",
    "#Punctuation removal for lemmatization\n",
    "def remove_punctuation(text):\n",
    "    no_punct=\"\".join([c for c in text if c not in string.punctuation])\n",
    "    return no_punct\n",
    "\n",
    "#Iterate through all the cities that met the threshold and search twitter on all of the \n",
    "#milestone dates nspike is the collection of most frequent words for each city, \n",
    "#for higher level summarization. ttwe is tokenized version of every tweet for more \n",
    "#involved natural language analysis\n",
    "\n",
    "\n",
    "nspike=[]\n",
    "ttwe=[]\n",
    "\n",
    "\n",
    "for s in spikes:\n",
    "    da=0\n",
    "  \n",
    "    for d in s['dates']:\n",
    "        da+=1\n",
    "        #Print when iterating to track progress, as it takes a while\n",
    "        print('city',s['city'],'dates',d)\n",
    "        \n",
    "        #Format date from COVID file for twitter to accept and search twitter near that \n",
    "        #city and on that date\n",
    "        city=s['city']\n",
    "        state=s['state']\n",
    "        county=s['county']\n",
    "        date=datetime.datetime.strptime(d, '%m/%d/%Y').strftime('%Y-%m-%d')\n",
    "        searchstring='''near:\"{}\" within:50mi until:{}'''.format(city,date)\n",
    "        \n",
    "        #Frequency to store word counts and success count sets the cutoff for how many \n",
    "        #tweets per date. Success is finding a tweet with a user who has tagged the city \n",
    "        #in question as their location and writes their tweet in English\n",
    "        \n",
    "        success=0\n",
    "        frequency={}\n",
    "                \n",
    "        \n",
    "        for i,tweet in enumerate(sntwitter.TwitterSearchScraper(searchstring).get_items()):\n",
    "            if success>1000:\n",
    "                break\n",
    "            else:\n",
    "                if tweet.lang =='en' and (tweet.user.location==city or tweet.user.location==state):\n",
    "                    success+=1\n",
    "                    \n",
    "                    #Frequency derived by tokenizing, removing stopwords, and lemmatizing\n",
    "                    \n",
    "                    tokens = nltk.word_tokenize(tweet.content)\n",
    "                    clean_tokens=[tok for tok in tokens if len(tok.lower())>1 and \n",
    "                                  (tok.lower() not in set(stopwords.words('english')))]\n",
    "\n",
    "                    cleanTok=[]\n",
    "                    for w in clean_tokens:\n",
    "                        cleanT = remove_punctuation(w).lower()\n",
    "                        if len(cleanT.strip())>1:\n",
    "                            cleanTok.append(cleanT.strip())\n",
    "\n",
    "                    lemmatizer = WordNetLemmatizer()\n",
    "                    textLemma=[]\n",
    "                    for tok in cleanTok:\n",
    "                        textLemma.append(lemmatizer.lemmatize(tok))\n",
    "                        if 'http' in textLemma: textLemma.remove('http')\n",
    "                            \n",
    "                    #In addition to lemmatizing for TF-IDF analysis, simple processing for \n",
    "                    #use with Word2Vec embeddings\n",
    "                    \n",
    "                    w2vtok=simple_preprocess(tweet.content, deacc=True)\n",
    "                    \n",
    "                    ttwe.append({'city':city,'state':state,'date':date,'threshold':da,\n",
    "                                 'tokenized_text':w2vtok,'lemma':textLemma})\n",
    "                    \n",
    "                    for w in textLemma:\n",
    "                        if frequency.get(w,False):\n",
    "                            frequency[w]+=1\n",
    "                        else:\n",
    "                            frequency[w]=1\n",
    "        \n",
    "        #Set cutoff for number of words included in frequency\n",
    "        sorted_freq_dist= sorted(frequency.items(), key=operator.itemgetter(1), reverse=True)\n",
    "        \n",
    "        \n",
    "        \n",
    "        for sf in sorted_freq_dist[:50]:\n",
    "            nspike.append({'city':city,'county':county,'state':state,'date':date,'threshold':da,'word':sf[0],\n",
    "                           'frequency':sf[1]})\n",
    "            \n",
    "storage = open('FFF.json','w')\n",
    "storagetwe = open('TTT.json','w')\n",
    "json.dump(ttwe,storagetwe)\n",
    "storagetwe.close()\n",
    "json.dump(nspike,storage)\n",
    "storage.close()        \n",
    "\n",
    "\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-7b73bc2aa2b7>:170: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  model_vector=(np.mean([threshmodel[token] for token in row ['tokenized_text']],axis=0)).tolist()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Likeliehood of predicting correct threshold at random: 0.3333\n",
      "Likeliehood of predicting correct state at random: 0.0667\n",
      "DecisionTree Threshold F1: 0.3283, -0.0050 better than random\n",
      "DecisionTree State F1: 0.0662, -0.0004 better than random\n",
      "DecisionTree Threshold TFID F1: 0.3820, 0.0486 better than random\n",
      "DecisionTree State TFID F1: 0.1534, 0.0867 better than random\n",
      "Random Forest Threshold F1: 0.3354, 0.0021 better than random\n",
      "Random Forest State F1: 0.0674, 0.0007 better than random\n",
      "Random Forest Threshold TFID F1: 0.3945, 0.0612 better than random\n",
      "Random Forest State TFID F1: 0.1774, 0.1108 better than random\n",
      "SGD Threshold F1: 0.3347, 0.0014 better than random\n",
      "SGD State F1: 0.0702, 0.0035 better than random\n",
      "SGD Threshold TFID F1: 0.3708, 0.0375 better than random\n",
      "SGD State TFID F1: 0.1588, 0.0921 better than random\n"
     ]
    }
   ],
   "source": [
    "#Part 2: NLP of collected twitter data to determine possible indicators for COVID in population, with \n",
    "#parallel analysis of twitter language by state to see if the amount of COVID cases is more or less of \n",
    "#a predictor of language than just the region where the language is from\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import TfidfModel\n",
    "from gensim import corpora\n",
    "import gensim\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except ModuleNotFoundError:\n",
    "    import pickle\n",
    "\n",
    "def save_object(obj, filename):\n",
    "    with open(filename, 'wb') as output:  # Overwrites any existing file.\n",
    "        pickle.dump(obj, output, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "#Earlier version with 4 threshold levels instead of 3 was downloaded in 4 different files \n",
    "#to save time. Manually filter second threshold so only dates are the 1K COVID case mark, \n",
    "#30K case mark, and 100K case mark. Load to ctweets dataframe for NLP\n",
    "\n",
    "#with open('TT1.json') as infile:\n",
    "    df1 = pd.DataFrame(json.load(infile))\n",
    "#with open('TT2.json') as infile:\n",
    "#    df2 = pd.DataFrame(json.load(infile))\n",
    "#with open('TT4.json') as infile:\n",
    "#    df4 = pd.DataFrame(json.load(infile))\n",
    "#with open('TT3.json') as infile:\n",
    "#    df3 = pd.DataFrame(json.load(infile))\n",
    "    \n",
    "with open('TTT.json') as infile:\n",
    "    ctweets1 = pd.DataFrame(json.load(infile))\n",
    "\n",
    "#Conversion process to single data frame and saved to pickle\n",
    "#Pickles are used for the dataset, the vectorization models and each of the estimator \n",
    "#classes because the code was running simultaneously in different notebooks training \n",
    "#different estimators to reduce processing time\n",
    "\n",
    "ctweets1=pd.concat([df1,df2,df3,df4])\n",
    "ctweets2=ctweets1[ctweets1[\"threshold\"]!=2]\n",
    "ctweets=ctweets2[ctweets2[\"tokenized_text\"].str.len()>0]\n",
    "#save_object(ctweets,\"ctweetspickle.pkl\")\n",
    "\n",
    "#with open(r\"ctweetspickle.pkl\", \"rb\") as input_file:\n",
    "#    ctweets = pickle.load(input_file)\n",
    "\n",
    "#The data set for measuring language at each date is uniformly distributed across the \n",
    "#time periods for that city although the dates themselves vary. The states however need \n",
    "#to be balanced because states like California and Texas are overweighted, so the dataset \n",
    "#for measuring by state is created so all the states have the same number\n",
    "#of records as the state with the smallest amount\n",
    "    \n",
    "    \n",
    "g = ctweets.groupby(['state'])\n",
    "g.size().reset_index(name='counts')\n",
    "f=g.apply(lambda x: x.sample(g.size().min()).reset_index())\n",
    "\n",
    "ctweetstate=f.droplevel(0).reset_index()\n",
    "\n",
    "\n",
    "\n",
    "#To deteremine whether there is more predictive accuracy based on the location of the \n",
    "#tweet or the timeline of COVID cases, there will be analysis run for date targets \n",
    "#(threshold) and location targets (state)\n",
    "\n",
    "\n",
    "\n",
    "X_train_threshold,X_test_threshold,\n",
    "y_train_threshold,y_test_threshold=train_test_split(\n",
    "    ctweets[[\"tokenized_text\",\"lemma\"]],ctweets[\"threshold\"],test_size=0.2,random_state=15)\n",
    "\n",
    "X_train_state,X_test_state,\n",
    "y_train_state,y_test_state=train_test_split(ctweetstate[[\"tokenized_text\",\"lemma\"]],\n",
    "                                            ctweetstate[\"state\"],test_size=0.2,\n",
    "                                            random_state=15)\n",
    "\n",
    "#Record the likelihood of a random guess to compare to later predicted results\n",
    "threshold_odds=1/ctweets[\"threshold\"].nunique()\n",
    "state_odds=1/ctweetstate[\"state\"].nunique()\n",
    "\n",
    "\n",
    "#Setup first for Word2Vec vectorization, and then bag of words vectorization\n",
    "#Size for both vectorizations limited to 1000 columns for states and thresholds\n",
    "\n",
    "threshtokens=pd.Series(ctweets[\"tokenized_text\"])\n",
    "threshmodel=Word2Vec(threshtokens,size=1000,window=6,min_count=1,workers=7,sg=1)\n",
    "#save_object(threshmodel,\"threshmodelpickle.pkl\")\n",
    "\n",
    "with open(r\"threshmodelpickle.pkl\", \"rb\") as input_file:\n",
    "    threshmodel = pickle.load(input_file)\n",
    "\n",
    "threshmydict=corpora.Dictionary(ctweets[\"lemma\"],prune_at=1000)\n",
    "threshcorpus=[threshmydict.doc2bow(line) for line in ctweets[\"lemma\"]]\n",
    "threshtfidf_model=TfidfModel(threshcorpus)\n",
    "save_object(threshtfidf_model,\"threshtfidf_modelpickle.pkl\")\n",
    "\n",
    "\n",
    "#Models for state\n",
    "\n",
    "statetokens=pd.Series(ctweetstate[\"tokenized_text\"])\n",
    "statemodel=Word2Vec(statetokens,size=1000,window=6,min_count=1,workers=7,sg=1)\n",
    "#save_object(statemodel,\"statemodelpickle.pkl\")\n",
    "\n",
    "statemydict=corpora.Dictionary(ctweetstate[\"lemma\"],prune_at=1000)\n",
    "statecorpus=[statemydict.doc2bow(line) for line in ctweetstate[\"lemma\"]]\n",
    "statetfidf_model=TfidfModel(statecorpus)\n",
    "#Save_object(statetfidf_model,\"statetfidf_modelpickle.pkl\")\n",
    "\n",
    "\n",
    "#Write vectorizations of X_train lists and X_test lists to four CSV documents \n",
    "#(eight including the state dataset) for repeated access later. FIrst the TFIDF vectorization, \n",
    "#then Word2Vec for train then test etc. In all cases setting index0 as header variable \n",
    "#because index value has been randomized\n",
    "\n",
    "\n",
    "vocab_len=len(threshmydict.token2id)\n",
    "index0=True\n",
    "tfid_tf='twittertrainXTFID.csv'\n",
    "with open(tfid_tf,'w+',encoding='utf8') as tfidf_file:\n",
    "    for index, row in X_train_threshold.iterrows():\n",
    "        doc=mydict.doc2bow(row['lemma'])\n",
    "        features = gensim.matutils.corpus2csc([threshtfidf_model[doc]],\n",
    "                                              num_terms=vocab_len).toarray()[:,0]\n",
    "        if index0:\n",
    "            header=\",\".join(str(threshmydict[ele]) for ele in range(vocab_len))\n",
    "            tfidf_file.write(header)\n",
    "            tfidf_file.write(\"\\n\")\n",
    "            index0=False\n",
    "        line1=\",\".join([str(vector_element) for vector_element in features])\n",
    "        tfidf_file.write(line1)\n",
    "        tfidf_file.write(\"\\n\")\n",
    "\n",
    "index0=True\n",
    "word2vec_tf=\"twittertrainX.csv\"\n",
    "with open(word2vec_tf,'w+')as word2vec_file:\n",
    "    for index, row in X_train_threshold.iterrows():\n",
    "\n",
    "        model_vector=(np.mean([threshmodel[token] for token in row ['tokenized_text']],\n",
    "                              axis=0)).tolist()\n",
    "        if index0:\n",
    "            index0=False\n",
    "            header=\",\".join(str(ele) for ele in range(1000))\n",
    "            word2vec_file.write(header)\n",
    "            word2vec_file.write(\"\\n\")\n",
    "        if type(model_vector) is list:\n",
    "            line1=\",\".join([str(vector_element) for vector_element in model_vector])\n",
    "        else:\n",
    "            line1=\",\".join([str(0) for i in range(1000)])\n",
    "        word2vec_file.write(line1)\n",
    "        word2vec_file.write('\\n')\n",
    "\n",
    "\n",
    "\n",
    "index0=True\n",
    "tfid_tf_test='twittertestXTFID.csv'\n",
    "with open(tfid_tf_test,'w+',encoding='utf8') as tfidf_file:\n",
    "    for index, row in X_test_threshold.iterrows():\n",
    "        doc=threshmydict.doc2bow(row['lemma'])\n",
    "        features = gensim.matutils.corpus2csc([threshtfidf_model[doc]],\n",
    "                                              num_terms=vocab_len).toarray()[:,0]\n",
    "        if index0:\n",
    "            header=\",\".join(str(threshmydict[ele]) for ele in range(vocab_len))\n",
    "            tfidf_file.write(header)\n",
    "            tfidf_file.write(\"\\n\")\n",
    "            index0=False\n",
    "        line1=\",\".join([str(vector_element) for vector_element in features])\n",
    "        tfidf_file.write(line1)\n",
    "        tfidf_file.write(\"\\n\")\n",
    "        \n",
    "index0=True\n",
    "word2vec_tf_test=\"twittertestX.csv\"\n",
    "with open(word2vec_tf_test,'w+')as word2vec_file:\n",
    "    for index, row in X_test_threshold.iterrows():\n",
    "\n",
    "        model_vector=(np.mean([threshmodel[token] for token in row ['tokenized_text']],\n",
    "                              axis=0)).tolist()\n",
    "        if index0:\n",
    "            index0=False\n",
    "            header=\",\".join(str(ele) for ele in range(1000))\n",
    "            word2vec_file.write(header)\n",
    "            word2vec_file.write(\"\\n\")\n",
    "        if type(model_vector) is list:\n",
    "            line1=\",\".join([str(vector_element) for vector_element in model_vector])\n",
    "        else:\n",
    "            line1=\",\".join([str(0) for i in range(1000)])\n",
    "        word2vec_file.write(line1)\n",
    "        word2vec_file.write('\\n')\n",
    "\n",
    "\n",
    "\n",
    "#Files for the state data set\n",
    "\n",
    "\n",
    "vocab_len=len(statemydict.token2id)\n",
    "index0=True\n",
    "statetfid_tf='statetwittertrainXTFID.csv'\n",
    "with open(statetfid_tf,'w+',encoding='utf8') as tfidf_file:\n",
    "    for index, row in X_train_state.iterrows():\n",
    "        doc=statemydict.doc2bow(row['lemma'])\n",
    "        features = gensim.matutils.corpus2csc([statetfidf_model[doc]],\n",
    "                                              num_terms=vocab_len).toarray()[:,0]\n",
    "        if index0:\n",
    "            header=\",\".join(str(statemydict[ele]) for ele in range(vocab_len))\n",
    "            tfidf_file.write(header)\n",
    "            tfidf_file.write(\"\\n\")\n",
    "            index0=False\n",
    "        line1=\",\".join([str(vector_element) for vector_element in features])\n",
    "        tfidf_file.write(line1)\n",
    "        tfidf_file.write(\"\\n\")\n",
    "\n",
    "index0=True\n",
    "stateword2vec_tf=\"statetwittertrainX.csv\"\n",
    "with open(stateword2vec_tf,'w+')as word2vec_file:\n",
    "    for index, row in X_train_state.iterrows():\n",
    "\n",
    "        model_vector=(np.mean([statemodel[token] for token in row ['tokenized_text']],\n",
    "                              axis=0)).tolist()\n",
    "        if index0:\n",
    "            index0=False\n",
    "            header=\",\".join(str(ele) for ele in range(1000))\n",
    "            word2vec_file.write(header)\n",
    "            word2vec_file.write(\"\\n\")\n",
    "        if type(model_vector) is list:\n",
    "            line1=\",\".join([str(vector_element) for vector_element in model_vector])\n",
    "        else:\n",
    "            line1=\",\".join([str(0) for i in range(1000)])\n",
    "        word2vec_file.write(line1)\n",
    "        word2vec_file.write('\\n')\n",
    "\n",
    "\n",
    "\n",
    "index0=True\n",
    "statetfid_tf_test='statetwittertestXTFID.csv'\n",
    "with open(statetfid_tf_test,'w+',encoding='utf8') as tfidf_file:\n",
    "    for index, row in X_test_state.iterrows():\n",
    "        doc=statemydict.doc2bow(row['lemma'])\n",
    "        features = gensim.matutils.corpus2csc([statetfidf_model[doc]],\n",
    "                                              num_terms=vocab_len).toarray()[:,0]\n",
    "        if index0:\n",
    "            header=\",\".join(str(statemydict[ele]) for ele in range(vocab_len))\n",
    "            tfidf_file.write(header)\n",
    "            tfidf_file.write(\"\\n\")\n",
    "            index0=False\n",
    "        line1=\",\".join([str(vector_element) for vector_element in features])\n",
    "        tfidf_file.write(line1)\n",
    "        tfidf_file.write(\"\\n\")\n",
    "        \n",
    "index0=True\n",
    "stateword2vec_tf_test=\"statetwittertestX.csv\"\n",
    "with open(stateword2vec_tf_test,'w+')as word2vec_file:\n",
    "    for index, row in X_test_state.iterrows():\n",
    "\n",
    "        model_vector=(np.mean([statemodel[token] for token in row ['tokenized_text']],\n",
    "                              axis=0)).tolist()\n",
    "        if index0:\n",
    "            index0=False\n",
    "            header=\",\".join(str(ele) for ele in range(1000))\n",
    "            word2vec_file.write(header)\n",
    "            word2vec_file.write(\"\\n\")\n",
    "        if type(model_vector) is list:\n",
    "            line1=\",\".join([str(vector_element) for vector_element in model_vector])\n",
    "        else:\n",
    "            line1=\",\".join([str(0) for i in range(1000)])\n",
    "        word2vec_file.write(line1)\n",
    "        word2vec_file.write('\\n')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Files stored with vectorizations of tweets\n",
    "#Now read into data frames, one each for Word2Vec method and bag of words method\n",
    "        \n",
    "word2vec_X=pd.read_csv(word2vec_tf)\n",
    "stateword2vec_X=pd.read_csv(stateword2vec_tf)\n",
    "\n",
    "tfid_X=pd.read_csv(tfid_tf)\n",
    "statetfid_X=pd.read_csv(statetfid_tf)\n",
    "\n",
    "test_features_X=pd.read_csv(word2vec_tf_test)\n",
    "statetest_features_X=pd.read_csv(stateword2vec_tf_test)\n",
    "\n",
    "test_features_tfid=pd.read_csv(tfid_tf_test) \n",
    "statetest_features_tfid=pd.read_csv(statetfid_tf_test) \n",
    "\n",
    "    \n",
    "#Initialize and train three classifiers, each for predicting the threshold and state, \n",
    "#and using the Word2Vec and BOW vectorizations\n",
    "    \n",
    "forest_thresh=RandomForestClassifier()\n",
    "forest_state=RandomForestClassifier()\n",
    "forest_thresh_tfid=RandomForestClassifier()\n",
    "forest_state_tfid=RandomForestClassifier()\n",
    "    \n",
    "dt_thresh=DecisionTreeClassifier()\n",
    "dt_state=DecisionTreeClassifier()\n",
    "dt_thresh_tfid=DecisionTreeClassifier()\n",
    "dt_state_tfid=DecisionTreeClassifier()\n",
    "\n",
    "sgd_thresh=SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=1000)\n",
    "sgd_state=SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=1000)\n",
    "sgd_thresh_tfid=SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=1000)\n",
    "sgd_state_tfid=SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=1000)\n",
    "\n",
    "forest_thresh.fit(word2vec_X,y_train_threshold)\n",
    "#save_object(forest_thresh,\"forest_threshpickle.pkl\")\n",
    "\n",
    "forest_state.fit(stateword2vec_X,y_train_state)\n",
    "#save_object(forest_state,\"forest_statepickle.pkl\")\n",
    "\n",
    "forest_thresh_tfid.fit(tfid_X,y_train_threshold)\n",
    "#save_object(forest_thresh_tfid,\"forest_thresh_tfidpickle.pkl\")\n",
    "\n",
    "forest_state_tfid.fit(statetfid_X,y_train_state)\n",
    "#save_object(forest_state_tfid,\"forest_state_tfidpickle.pkl\")\n",
    "\n",
    "dt_thresh.fit(word2vec_X,y_train_threshold)\n",
    "#save_object(dt_thresh,\"dt_threshpickle.pkl\")\n",
    "\n",
    "dt_state.fit(stateword2vec_X,y_train_state)\n",
    "#save_object(dt_state,\"dt_statepickle.pkl\")\n",
    "\n",
    "dt_thresh_tfid.fit(tfid_X,y_train_threshold)\n",
    "#save_object(dt_thresh_tfid,\"dt_thresh_tfidpickle.pkl\")\n",
    "\n",
    "dt_state_tfid.fit(statetfid_X,y_train_state)\n",
    "#save_object(dt_state_tfid,\"dt_state_tfidpickle.pkl\")\n",
    "\n",
    "sgd_thresh.fit(word2vec_X,y_train_threshold)\n",
    "#save_object(sgd_thresh,\"sgd_threshpickle.pkl\")\n",
    "\n",
    "sgd_state.fit(stateword2vec_X,y_train_state)\n",
    "#save_object(sgd_state,\"sgd_statepickle.pkl\")\n",
    "\n",
    "sgd_thresh_tfid.fit(tfid_X,y_train_threshold)\n",
    "#save_object(sgd_thresh_tfid,\"sgd_thresh_tfidpickle.pkl\")\n",
    "\n",
    "sgd_state_tfid.fit(statetfid_X,y_train_state)\n",
    "#save_object(sgd_state_tfid,\"sgd_state_tfidpickle.pkl\")\n",
    "\n",
    "\n",
    "\n",
    "#Run test features through trained models for each of the estimators. Because the classes\n",
    "#for the date threshold and the state are both non-binary, the precision, recall \n",
    "#and f1 scores are all the same\n",
    "\n",
    "#If there is enough RAM, it is not necessary to load from pickles and delete after prediction\n",
    "\n",
    "\n",
    "#with open(r\"sgd_threshpickle.pkl\", \"rb\") as input_file:\n",
    "#    sgd_thresh = pickle.load(input_file)\n",
    "\n",
    "#with open(r\"sgd_statepickle.pkl\", \"rb\") as input_file:\n",
    "#    sgd_state = pickle.load(input_file)\n",
    "\n",
    "test_predictions_thresh= sgd_thresh.predict(test_features_X)\n",
    "test_predictions_state= sgd_state.predict(statetest_features_X)\n",
    "    \n",
    "#del sgd_thresh\n",
    "#del sgd_state\n",
    "    \n",
    "    \n",
    "#with open(r\"sgd_thresh_tfidpickle.pkl\", \"rb\") as input_file:\n",
    "#    sgd_thresh_tfid = pickle.load(input_file)\n",
    "\n",
    "#with open(r\"sgd_state_tfidpickle.pkl\", \"rb\") as input_file:\n",
    "#    sgd_state_tfid = pickle.load(input_file)\n",
    "\n",
    "\n",
    "sgdtfid_threshpredict=sgd_thresh_tfid.predict(test_features_tfid)\n",
    "sgdtfid_statepredict=sgd_state_tfid.predict(statetest_features_tfid)\n",
    "\n",
    "#del sgd_state_tfid\n",
    "#del sgd_thresh_tfid\n",
    "\n",
    "\n",
    "#with open(r\"dt_threshpickle.pkl\", \"rb\") as input_file:\n",
    "#    dt_thresh = pickle.load(input_file)\n",
    "\n",
    "#with open(r\"dt_statepickle.pkl\", \"rb\") as input_file:\n",
    "#    dt_state = pickle.load(input_file)\n",
    "    \n",
    "treep_thresh=dt_thresh.predict(test_features_X)\n",
    "treep_state=dt_state.predict(statetest_features_X)\n",
    "\n",
    "#del dt_thresh\n",
    "#del dt_state\n",
    "\n",
    "#with open(r\"dt_thresh_tfidpickle.pkl\", \"rb\") as input_file:\n",
    "#    dt_thresh_tfid = pickle.load(input_file)\n",
    "\n",
    "#with open(r\"dt_state_tfidpickle.pkl\", \"rb\") as input_file:\n",
    "#    dt_state_tfid = pickle.load(input_file)\n",
    "\n",
    "\n",
    "dt_tfid_thresh_pred=dt_thresh_tfid.predict(test_features_tfid)\n",
    "dt_tfid_state_pred=dt_state_tfid.predict(statetest_features_tfid)\n",
    "\n",
    "#del dt_thresh_tfid\n",
    "#del dt_state_tfid\n",
    "\n",
    "\n",
    "#with open(r\"forest_threshpickle.pkl\", \"rb\") as input_file:\n",
    "#    forest_thresh = pickle.load(input_file)\n",
    "    \n",
    "forest_thresh_predict=forest_thresh.predict(test_features_X)\n",
    "\n",
    "#del forest_thresh\n",
    "\n",
    "#with open(r\"forest_statepickle.pkl\", \"rb\") as input_file:\n",
    "#    forest_state = pickle.load(input_file)\n",
    "\n",
    "forest_state_predict=forest_state.predict(statetest_features_X)\n",
    "\n",
    "#del forest_state\n",
    "\n",
    "#with open(r\"forest_thresh_tfidpickle.pkl\", \"rb\") as input_file:\n",
    "#    forest_thresh_tfid = pickle.load(input_file)\n",
    "\n",
    "forest_thresh_tfid_predict=forest_thresh_tfid.predict(test_features_tfid)\n",
    "\n",
    "#del forest_thresh_tfid\n",
    "    \n",
    "#with open(r\"forest_state_tfidpickle.pkl\", \"rb\") as input_file:\n",
    "#    forest_state_tfid = pickle.load(input_file)\n",
    "\n",
    "forest_state_tfid_predict=forest_state_tfid.predict(statetest_features_tfid)\n",
    "\n",
    "#Results for training methods and targets compared to random likelihood\n",
    "\n",
    "\n",
    "print(\"Likeliehood of predicting correct threshold at random: {:.4f}\".format(threshold_odds))\n",
    "print(\"Likeliehood of predicting correct state at random: {:.4f}\".format(state_odds))\n",
    "\n",
    "f1 = f1_score(y_test_threshold,treep_thresh,average=\"micro\")\n",
    "print(\"DecisionTree Threshold F1: {:.4f}, {:.4f} better than random\".format(f1, \n",
    "                                                                            (f1-threshold_odds)))\n",
    "\n",
    "f1 = f1_score(y_test_state,treep_state,average=\"micro\")\n",
    "print(\"DecisionTree State F1: {:.4f}, {:.4f} better than random\".format(f1, (f1-state_odds)))\n",
    "\n",
    "f1 = f1_score(y_test_threshold,dt_tfid_thresh_pred,average=\"micro\")\n",
    "print(\"DecisionTree Threshold TFID F1: {:.4f}, {:.4f} better than random\".format(f1, \n",
    "                                                                                 (f1-threshold_odds)))\n",
    "\n",
    "f1 = f1_score(y_test_state,dt_tfid_state_pred,average=\"micro\")\n",
    "print(\"DecisionTree State TFID F1: {:.4f}, {:.4f} better than random\".format(f1, \n",
    "                                                                             (f1-state_odds)))\n",
    "\n",
    "f1 = f1_score(y_test_threshold,forest_thresh_predict,average=\"micro\")\n",
    "print(\"Random Forest Threshold F1: {:.4f}, {:.4f} better than random\".format(f1, \n",
    "                                                                             (f1-threshold_odds)))\n",
    "\n",
    "f1 = f1_score(y_test_state,forest_state_predict,average=\"micro\")\n",
    "print(\"Random Forest State F1: {:.4f}, {:.4f} better than random\".format(f1, (f1-state_odds)))\n",
    "\n",
    "f1 = f1_score(y_test_threshold,forest_thresh_tfid_predict,average=\"micro\")\n",
    "print(\"Random Forest Threshold TFID F1: {:.4f}, {:.4f} better than random\".format(f1, \n",
    "                                                                                  (f1-threshold_odds)))\n",
    "\n",
    "f1 = f1_score(y_test_state,forest_state_tfid_predict,average=\"micro\")\n",
    "print(\"Random Forest State TFID F1: {:.4f}, {:.4f} better than random\".format(f1, \n",
    "                                                                              (f1-state_odds)))\n",
    "\n",
    "f1 = f1_score(y_test_threshold,test_predictions_thresh,average=\"micro\")\n",
    "print(\"SGD Threshold F1: {:.4f}, {:.4f} better than random\".format(f1, (f1-threshold_odds)))\n",
    "\n",
    "f1 = f1_score(y_test_state,test_predictions_state,average=\"micro\")\n",
    "print(\"SGD State F1: {:.4f}, {:.4f} better than random\".format(f1, (f1-state_odds)))\n",
    "\n",
    "f1 = f1_score(y_test_threshold,sgdtfid_threshpredict,average=\"micro\")\n",
    "print(\"SGD Threshold TFID F1: {:.4f}, {:.4f} better than random\".format(f1, \n",
    "                                                                        (f1-threshold_odds)))\n",
    "\n",
    "f1 = f1_score(y_test_state,sgdtfid_statepredict,average=\"micro\")\n",
    "print(\"SGD State TFID F1: {:.4f}, {:.4f} better than random\".format(f1, (f1-state_odds)))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
